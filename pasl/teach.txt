An Introduction to Parallel Computing in C++ 
============================================
Umut A. Acar and Arthur Chargueraud and Mike Rainey
v1.2 2016-03

:doctype: book
:imagesdir: ../originals
:toc:
:cpp: {basebackend@docbook:c++:cpp}
:numbered:

= Chapter: Teaching Notes

What follows is an outline of the 4 lectures on PASL. 

== Lecture 1


=== Introduction


. Broader perspective: so far, focused on design of parallel
algorithms.  Implementation in SML, could execute some programs in
parallel.  There are many other programming languages or extensions
for parallel programming.  Unlike SML, many of these languages are
based on imperative languages such as C/Clatexmath:[++] and Java.  In the next two
weeks, we shall see one such approach based on C++.  

As you will see, programming parallel algorithms in imperative
languages requires paying attention to many more implementation
details to ensure correctness.  We shall examples of this.  As you
will see many of these are not germane to algorithm design and can be
viewed as distractions.  

. We will use the Clatexmath:[++] language and a library, called PASL,
for writing parallel code in Clatexmath:[++].

. Ask students if they are familiar with  Clatexmath:[++] and
recommend them to learn it if they are not.  They don't need to be
using anything deep from this language but they will definitely
need some familiarity.


. Empirical concerns: This being an algorithms class, we paid little
attention to empirical concerns and practical performance.  While we
hope you to see to learn more about these in your later courses, we
don't want you to walk away with the false impression that practical
issues are trivial.  They are not.  Especially in parallel algorithms,
practical efficiency is very important and can require careful
algorithm engineering. In the next two weeks, we will cover the basic
vocabulary of empirical efficiency and see how we might write good
code in practice.  

. Concurrency: So far we only covered parallelism.  An important
related topic in computer science is concurrency.  While concurrency
is a largely a separate topic, it can be very useful in the design of
"parallel" algorithms.  We shall cover some basic concepts of
concurrency and how they can be used to implement parallel
algorithms.

. Lab: There will be a lab called "PASL Lab" based on the material here.
Based on the feedback that we got from last years, we can say that
this is one of the labs that the students enjoyed.  It is challenging
but you will learn a lot.  So it is a highly recommended lab.



=== Templates


. One of features of Clatexmath:[++] that we shall use a lot is
templates.  Templates correspond to parametric polymorhism in SML.  

. Give the latexmath:[$\alpha$]  `array` example and show how this
corresponds to the `array<T>`.

. Give the identity function in SML with type latexmath:[$\alpha
\rightarrow \alpha$] and how this corresponds to the function given i
the notes.

=== Concurrency and Fork-Join Parallelism

. In this class, what you have learned is the fork-join parallelism.  

. There are two other approaches: async-finish and
futures. Async-finish is very similar to fork-join.  Futures is a bit
different because it makes parallel computations first-class values.
Futures can be more powerful.

. Draw a spectrum from fork join to general concurrency.

. Draw some DAG's to illustrate these.

. A more general notion of multithreading also exists but this is
really like programming with GOTO's.

.  Talk about PASL's `fork2`.  Mention that the argument are unit
arrow unit functions.  This means that there is no return value.
Returns must be written to memory.


=== Race Conditions

.  Talk about the examples.  For the first example, ask what
happens if you printed the values from other branch inside each
branch.  Highlight the nondeterministic behavior.  Talk about how
these two can really be executed in any order.  THIS IS NOT IN THE
NOTES.

. For the `result +=` example, ask them to find the race after
claiming that there is no race because they are both writing.  Point
out that you lied aftewards.

. Talk about compare and swap and other atomic operations.

. Ask them as a puzzle how to fix the `result +=` example using
compare and swap.

. Talk about atomic memory.

Benchmarking
^^^^^^^^^^^^

Follow the <<ch:benchmarking, this chapter>>

Work efficiency
^^^^^^^^^^^^^^^

. Asymptotic work efficiency
. Observed work efficiency 


=== Automatic Granularity Control
Follow the notes

See the review below for a nice outline of this section.

				  
== Lecture 2

=== Review

. Show the code for and run sequential array increment . Show the code
for and run parallel array increment, proc 1, full parallel.

. Highlight the difference in run-time and ask why this is due.

. Draw a spectrum, sequential algorithm on one end, parallel on the on
other and talk about the gap.  One reason for the gap could be
asymptotic. This does not exist here.  They are both linear. Recall that we called parallel
algorithms that have the same asymptotic complexity as sequential
*_asymptotically work efficient_*.    Another
reason could be constant factors. We can see this by comparing the
elision and the parallel and the sequential and the elision.  Talk
about these.  

. What is the reason for the gap between parallel and
elision.  It is due to the cost of fork2.  The point is that fork2 is
expensive.  It does not have to be this expensive.  The expense can be
reduced but not by much.  

. What can we do? We talked about how granularity control can help.
The C++ library that you will be using, PASL, has built-in support for
automatic granularity control.  There is some syntactic overhead for
doing this, but as a result you get provable performance.

. Let's see the code for this, using pfor.  This is a nice piece of
code that does everything for us.  

  .. Run the pfor code and see that it is really close to the serial.

  .. Run also a speedup experiment to make sure that it does not over
  sequentialize.

. We call programs like this *_observably work efficient_*.  We
distinguish between these two because not all parallel algorithms are
observably work efficient, even if they may be asymptotically work
efficient. We will see examples of this.  Sometimes, it requires quite
a bit of engineering work to design an observably work efficient
parallel algorithm.


=== Outline

. We shall cover the following topics
  .. A sequence library based on arrays
  .. An application: parallel sorting
  .. Graphs and BFS


=== Simple Parallel Arrays and Sorting

. A brief summary of sparrays.  An implementation of their array
sequence structure.

. Run quicksort as they have seen in the class

. Run the baseline, highly optimized implementation of in place
quicksort.  

. Asymptotically work efficient but not observably so.  The reason:
the many different arrays being created, left, right, equal and the
many passes over the data.  Whereas in place quicksort does one pass,
ours does 3 for each filter and plus 1 for the concat.  We can see
this by the difference, which approximately factor 3.


.  We might thus ask, well what if we do the filters sequentially in
place and perform the two calls in parallel.  This is a feasible
approach certainly. SHow the code.  Run the code.  See speedups.
Lesson learned: work efficiency seems to be important especially in
small scale.

. How about other algorithms?  Let's see merge sort.
  .. Show code for parallel merge sort
  .. Analyze sequential merge nlgn work n span
  .. Run experiments 100 million

=== Graphs and BFS

. Motivation for Graphs
. Graph Representation

  .. Compressed Arrays: edge array and vertex array
  .. Space: n+m
  .. Time: Puzzle. 
     ... find neighbors: O(1)
     ... outdegree: O(1), check the difference between the vertex and
  the next. 
           
  .. Show code for adjlist

. Graph creation and examples.

. BFS 
  .. Pseudocode for 210 Version of BFS
     ... Key point: no vertex is visited more than once.
  .. How to implement this efficiently on a Multicore?

     By efficiently what we mean is that it should be observably work
     efficient.  So there is one operation here that we don't use at
     all in serial BFS.  What is it? It is the unions and reduces.
    
  .. Give the proposed pseudocode.  The key point is that the frontier
  now is the most recently visited set of vertices.

  .. Is this algorithm parallel?
     ... Puzzle: How do we prevent multiple visits via outedges?
     ... Compare and Swap

  .. Edge Map

  .. Exercise: It is possible to represent the frontier somewhat more
  loosely to also allow vertices that are already visited but marked
  with a sentinel value as described in the text. Can this be
  adventageous?
   
== Lecture 3: Multithreading and Concurrency

. Multithreaded program, definition as dag
. Work and span
. Execution via scheduling: Example
. Scheduling lower bounds
. Scheduling upper bounds: offline scheduling
. Scheduling upper bounds: online scheduling
  .. Global queue
  .. Work stealing
. Writing multithreaded programs: PThreads
  .. Example
  .. Synchronization: join, mutex, condition variables
. Writing multithreaded programs: structured threading
  .. Fork join, async-finish, futures: Parallel ML, PASL, Cilk, X10
. Critical Sections and Mutual Exclusion
  . Define critical section: no more than one thread at a time
  . Requires mutual exclusion, if not, a race condition can occur
  . How to ensure mutual exclusion? 
    .. Approach 1: lock based approaches

       ... spin locks
       ... blocking locks      

       Blocking or not, locks are implemented as part of the language
       system.  For example, in pThreads, there are functions such as

       ----
       pthread_mutex_t my_mutex
       pthread_mutex_init ()
       pthread_mutex_destroy ()
       pthread_mutex_lock: mutex -> mutex (blocking)
       pthread_mutex_trylock: mutex -> status (non-blocking)
       pthread_mutex_unlock: mutex -> mutex
       ----       

    .. Approach 2: lock-free approach

       ... Nonblocking: the idea is to "take a picture" of the world,
       change the world locally, and use a compare and swap to commit  the
       changes to the world.  
        
== Structured Multithreading

==== Review

  . Multithreading: threads, each of which are sequential communicate.
    Can be represented as a dag. 
    Work and Span apply.

    Scheduling lower and upper bounds.

  . Writing multithreaded programs pthreads
  . Race conditions, writing to the screen example
  . Solved by using a BigLock
  . Implemented the BigLock with compare and swap
  
==== Today

   . Such complexity is not always necessary: many interesting
   multithreaded programs can be written by using more structured
   multithreading primitives.

   . Structured Multithreading = restrict the manner in which threads
   can communicate.  For example, threads can only synchronize through
   "join" rather than in arbitrary ways.

    This means that a thread can wait for another to complete and that
    is the only way for two threads to synchronize.

    There are other ways, for example async-finish and futures.  They
    all can be viewed as a restricted form of synchronization.

   . One way to run such programs is to "compile" them to
   pThreads. Grossly inefficent. pThreads coarse grained
   parallelism. Not suitable to millions of threads.      

   . Specific support developed in many languages such as C, X10,
   Java, ML, Haskell

   . Structured multithreading combines the best of both worlds. We
   can give sequential semantics and also make sure to execute
   efficiently.  

    . Somewhat informally we think of structured multithreading and
    parallelism to be the same thing. 

   . Parallelism = sequential semantics

   . Not all programs. Multithreaded stack example. Such programs are
   concurrent.

   . Parallelism = property of execution/hardware
   . Concurrency = property of application.

Executing parallel programs
^^^^^^^^^^^^^^^^^^^^^^^^^^^

SKIP THIS SECTION. WE SHALL COVER THIS MATERIAL LATER IN MULTITHREADED
PROGRAMMING.

. Example program: array increment.

. The basic idea: partition computation into "units", each of which
execute sequentially and map those that can be run in parallel. 
  
. A thread is a sequence of instructions that execute serially.  This
means that they don't include a `fork2`.

. Scheduling: map threads to processors/cores.

. When doing this, we have to observe dependencies.  So use DAG's.
Show example DAG.


. Offline scheduling: you are given a DAG and want to map the DAG to
processors.  

. Give the example schedule.

. Theorem: Any greedy schudule is good.

. Scheduler: computes this schedule.  Challenges:

  .. You don't know the DAG, it unfolds as it executes.

  .. It takes time to create threads, find work, and distribute.  This
  we call *_scheduling friction_*.

. Example scheduler: centralized scheduler.  Does not scale.

. Example scheduler: work stealing.  Works well. Minimal friction.
 



[index]
Index
-----

