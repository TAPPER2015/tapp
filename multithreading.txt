[[ch:multithreading, Multithreading]]
== Chapter: Multithreading, Parallelism, and Concurrency


The term *_multithreading_* refers to computing with multiple threads
of control. Once created, a thread performs a computation by executing
a sequence of instructions, as specified by the program, until it
terminates.  A multithreaded computation start by executing a *_main
thread_*, which is the thread at which the execution starts.  A thread
can create or *_spawn_* another thread and *_synchronize_* with other
threads by using a variety of synchronization constructs such as locks,
mutex's, synchronization variables, and semaphores.

=== DAG Representation

A multithreaded computation can be represented by a dag, a Directed
Acyclic Graph, or written also more simply a *_dag_*, of vertices. The
figure below show an example multithreaded computation and its dag.
Each vertex represents the execution of an *_instruction_*, such as an
addition, a multiplication, a memory operation, a (thread) spawn
operation, or a synchronization operation.  A vertex representing a
spawn operation has outdegree two.  A synchronization operation waits
for an operation belonging to a thread to complete, and thus a vertex
representing a synchronization operation has indegree two.  Recall
that a dag represents a partial order.  Thus the dag of the
computation represents the partial ordering of the dependencies
between the instructions in the computation.


[[multithreading::general-threads, Figure ?]]
.A multithreaded computation.
image::general-threads.png["A multithreaded computation.",width="500pt",align="center"]

Throughout this book, we make two assumptions about the structure of
the dag:

. Each vertex has outdegree at most two.

. The dag has exactly one *_root vertex_* with indegree zero and one
*_final vertex_* vertex with outdegree zero.  The root is the first
instruction of the *_root thread_*.

The outdegree assumption naturally follows by the fact that each
vertex represents an instruction, which can create at most one thread.


=== Cost Model: Work and Span

For analyzing the efficiency and performance of multithreaded
programs, we use several cost measures, the most important ones
include work and span.  We define the *_work_* of a computation as the
number of vertices in the dag and the *_span_* as the length of the
longest path in the dag.  In the example dag above, work is
latexmath:[$15$] and span is latexmath:[$9$].

=== Execution and Scheduling

Execution of a multithreaded computation executes the vertices in the
dag of the computation in some partial order that is consistent with
the partial order specified by the dag, that is, if vertices
latexmath:[$u,v$] are ordered then the execution orders them in the
same way.  


Multithreaded programs are executed by using a *_scheduler_* that
assigns vertices of the dag to processes.  

.Definition: Execution Schedule
****

Given a dag latexmath:[$G$], an (execution) schedule for latexmath:[$G$]
is a function from processes and (time) *_steps_* to instructions of
the dag such  that

. if a vertex latexmath:[$u$] is ordered before another
latexmath:[$v$] in latexmath:[$G$], then latexmath:[$v$] is not
executed at a time step before  latexmath:[$u$], and

. each vertex in latexmath:[$G$]  is executed exactly once.

The *_length_* of a schedule is the number of steps in the schedule.

****

The first condition ensures that a schedule observes the dependencies
in the dag.  Specifically, for each arc latexmath:[$(u,v)$] in the
dag, the vertex latexmath:[$u$] is executed before vertex
latexmath:[$v$].  


For any step in the execution, we call a vertex *_ready_* if all the
ancestors of the vertex in the dag are executed prior to that step.
Similarly, we say that a thread is ready if it contains a ready
vertex.  Note that a thread can contain only one ready vertex at any
time.

.Schedule
=====

An example  schedule with 3 processes. The length of this
schedule is latexmath:[$10$]

[width="100%",frame="topbot",options="header"]
|==========
|Time Step | Process 1    | Process 2    | Process 3 
|1         |     M1       |              |     
|2         |     M2       |              |     
|3         |     M3       |     A1       |             
|4         |              |     A2       |     
|5         |     B1       |     A3       |              
|6         |     B2       |     A4       |               
|7         |     B2       |              |     M4         
|8         |              |     A5       |     M5         
|9         |     A6       |              |              
|10        |              |     M6       |              
|==========

====



// Note: this definition does not seem to require specifying the
// processes executing an instruction.




[[fct:simple-invariant-scheduling]]
.Fact: Scheduling Invariant
*****

Consider a computation dag latexmath:[$G$] and consider an execution using any
scheduling algorithm.  At any time during the execution, color the
vertices that are executed as blue and the others as red.  

. The blue vertices induce a blue sub-dag of latexmath:[$G$] that is
connected and that has the same root as latexmath:[$G$].

. The red vertices incude a red sub-dag of latexmath:[$G$] that is connected.

. All the vertices of G are in the blue or the red sub-dag.  In other
words, the blue and red vertices partitions the dag into two
sub-dags.

*****



=== Scheduling Lower Bounds 


.Theorem: Lower bounds
********

Consider any multithreaded computation with work latexmath:[$W$] and
span latexmath:[$S$] and latexmath:[$P$] processes.  The following
lower bounds hold.

. Every execution schedule has length at least latexmath:[$\frac{W}{P}$].

. Every execution schedule has length at least latexmath:[$S$].

********

The first lower bound follows by the simple observation that a
schedule can only execute latexmath:[$P$] instructions at a time.
Since all vertices must be executed, a schedule has length at least
latexmath:[$\frac{W}{P}$].  The second lower bound follows by the
observation that the schedule cannot execute a vertex before its
ancestors and thus the length of the schedule is at least as long as
any path in the dag, which can be as large as the span latexmath:[$S$].

=== Offline Scheduling

Having established a lower bound, we now move on to establish an upper
bound for the *_offline scheduling problem_*, where we are given a dag
and wish to find an execution schedule that minimizes the run time.
It is known that the related decision problem in NP-complete but that
2-approximation is relatively easy.  We shall consider two distinct
schedulers: *_level-by-level scheduler_* and *_greedy scheduler_*.

A *_level-by-level schedule_* is a schedule that executes the
instructions in a given dag level order, where the *_level_* of a
vertex is the longest distance from the root of the dag to the vertex.
More specifically, the vertices in level 0 are executed first,
followed by the vertices in level 1 and so on.

.Theorem[Offline level-by-level schedule]
****

For a dag with work latexmath:[$W$] and span latexmath:[$S$], the
length of a level-by-level schedule is latexmath:[$W/P + S$].

****

.Proof
****

Let latexmath:[$W_i$] denote the work of the instructions at level
latexmath:[$i$].  These instructions can be executed in
latexmath:[$\lceil \frac{W_i}{P} \rceil$] steps.  Thus the total time is 

[latexmath]
+++++++++++
\[
\sum_{i=1}^{S}{\lceil \frac{W_i}{P} \rceil} 
\le 
\sum_{i=1}^{S}{\lfloor \frac{W_i}{P} \rfloor + 1}
\le 
\lfloor \frac{W}{P} \rfloor + S
\]
+++++++++++

****


This theorem called
https://www.google.com/search?q=Brent%27s+theorem&gws_rd=ssl [Brent's
theorem] was proved by Brent in 1974. It shows that the lower bound
can be approximated within a factor of latexmath:[$2$].

Brent's theorem has later been generalized to all greedy schedules.  A
*_greedy schedule_* is a schedule that never leaves a process idle
unless there is no ready vertices.  In other words, greedy schedules
keep processes as busy as possibly by greedily assigning ready
vertices. 



[[thm:greedy]]
.Theorem: Offline Greedy Schedule 
********

Consider a dag latexmath:[$G$] with work latexmath:[$W$] and span
latexmath:[$S$]. Any schedule latexmath:[$P$]-process schedule has
length at most latexmath:[$\frac{W}{P} + S \cdot \frac{P-1}{P}$].

********


.Proof
********

Consider any greedy schedule with length latexmath:[$T$].

For each step latexmath:[$1 \le i \le T$], and for each process that
is scheduled at that step, collect a token.  The token goes to the
*_work bucket_* if the process executes a vertex in that step,
otherwise the process is idle and the token goes to a *_idle bucket_*.

Since each token in the work bucket corresponds to an executed vertex,
there are exactly latexmath:[$W$] tokens is that bucket.  

We will now bound the tokens in the idle bucket by latexmath:[$S \cdot
(P-1)$].  Observe that at any step in the execution schedule, there is
a ready vertex to be executed (because otherwise the execution is
complete).  This means that at each step, at most latexmath:[$P-1$]
processes can contribute to the idle bucket.  Furthermore at each
step, where there is at least one idle process, we know that the
number of ready vertices is less than the number of available
processes.  Note now that at that step, all the ready vertices have no
incoming edges in the red sub-dag consisting of the vertices that are
not yet executed, and all the vertices that have no incoming edges in
the red sub-dag are ready.  Thus executing all the ready vertices at
the step reduces the length of all the paths that originate at these
vertices and end at the final vertex by one. This means that the span
of the red sub-dag is reduced by one because all paths with length
equal to span must originate in a ready vertex. Since the red-subdag
is initially equal to the dag latexmath:[$G$], its span is
latexmath:[$S$], and thus there are at most latexmath:[$S$] steps at
which a process is idle.  As a result the total number of tokens in
the idle bucket is at most $S \cdot (P-1)$].

Since we collect latexmath:[$P$] tokens in each step, the bound thus
follows.

********

.Exercise
****

Show that the bounds for Brent's level-by-level scheduler and for any
greedy scheduler is within a factor latexmath:[$2$] of optimal.

****

=== Online Scheduling

In offline scheduling, we are given a dag and are interested in
finding a schedule with minimal length.  When executing multithreaded
program, however, we don't have full knowledge of the dag.  Instead,
the dag unfolds as we run the program.  Furthermore, we are interested
in not minimizing the length of the schedule but also the work and
time it takes to compute the schedule.  These two additional
conditions define the *_online scheduling problem._*


There are many different algorithms for online scheduling but these
algorithms all operate similarly. We can outline a typical scheduling
algorithm as follows. The algorithm maintain a *_work pool_* of work,
consisting of ready threads, and execute them. Execution starts with
the root thread in the pool.  It ends when the final vertex is
executed.  In order to minimize the cost of computing the schedule,
the algorithm executes a thread until there is a need for
synchronization with other threads.

To obtain work, a process removes a thread from the pool and executes
its ready vertex. We refer to the thread executed by a process as the
*_assigned thread_*.  When executed, the ready vertex can make the
next vertex of the thread ready, which then also gets executed an so
on until one of the following *_synchronization_* actions occur.

. *_Die:_* The process executes last vertex of the thread, causing
the thread to die. The process then obtains other work.

. *_Block:_* The assigned vertex executes but the next vertex does not
become ready. This blocks the thread and thus the process obtains
other work.

. *_Enable:_* The assigned vertex makes ready the continuation of the
vertex and unblocks another previously blocked thread by making a
vertex from that thread ready. In this case, the process inserts both
(any) one thread into the work pool and continues to execute the
other.

. *_Spawn:_* The assigned vertex spaws another thread.  As in the
previous case, the process inserts one thread into the work pool and
continues to execute the other.

These actions are not mutually exclusive.  For example, a thread may
spawn/enable a thread and die.  In this case, the process performs the
corresponding steps for each action.
				   

.Exercise: Scheduling Invariant
****
Convince yourself that the scheduling invariant holds in online scheduling.
****

For a given schedule generated by an online scheduling algorithm, we
can define a tree of vertices, which tell us far a vertex, the vertex
that enabled it.

.Definition: Enabling Tree
****

Consider the execution of a dag.  If the execution of a vertex
latexmath:[$u$] enables another vertex latexmath:[$v$], then we call
the edge latexmath:[$(u,v)$] an *_enabling edge_* and we call
latexmath:[$u$] the *_enabling parent_* of latexmath:[$v$].  For
simplicity, we simply use the term *_parent_* instead of enabling
parent.

Note that any vertex other than the root vertex has one enabling
parent.  Thus the subgraph induced by the enabling edges is a rooted
tree that we call the *_enabling tree_*.  

****

=== Writing Multithreaded Programs: Pthreads


Multithreaded programs can be written using a variety of language
abstractions interfaces. One of the most widely used interfaces is the
*_POSIX Threads*_ or *_Pthreads_* interface, which specifies a
programming interface for a standardized C language in the IEEE POSIX
1003.1c standard. Pthreads provide a rich interface that enable the
programmer to create multiple threads of control that can synchronize
by using the nearly the whole range of the synchronization facilities
mentioned above.

.Hello world with Pthreads

An example Pthread program is shown below.  The main thread (executing
function `main`) creates 8 child threads and terminates.  Each child
in turn runs the function `helloWorld` and immediately
terminates. Since the main thread does not wait for the children to
terminate, it may terminate before the children does, depending on how
threads are scheduled on the available processors.

[source,{cpp}]
----
#include <iostream>
#include <cstdlib>
#include <pthread.h>

using namespace std;

#define NTHREADS 8

void *helloWorld(void *threadid)
{
   long tid;
   tid = (long)threadid;
   cout << "Hello world! It is me, 00" << tid << endl;
   pthread_exit(NULL);
}

int main ()
{
   pthread_t threads[NTHREADS];
   int rc;
   int i;
   for( i=0; i < NTHREADS; i++ ){
      cout << "main: creating thread 00" << i << endl;
      error = pthread_create(&threads[i], NULL, helloWorld, (void *) i);
      if (error) {
         cout << "Error: unable to create thread," << error << endl;
         exit(-1);
      }
   }
   pthread_exit(NULL);
}


----

When executed this program may print the following.

----
main: creating thread 000
main: creating thread 001
main: creating thread 002
main: creating thread 003
main: creating thread 004
main: creating thread 005
main: creating thread 006
main: creating thread 007
Hello world! It is me, 000
Hello world! It is me, 001
Hello world! It is me, 002
Hello world! It is me, 003
Hello world! It is me, 004
Hello world! It is me, 005
Hello world! It is me, 006
Hello world! It is me, 007
----

But that would be unlikely, a more likely output would look like this:


----
main: creating thread 000
main: creating thread 001
main: creating thread 002
main: creating thread 003
main: creating thread 004
main: creating thread 005
main: creating thread 006
main: creating thread 007
Hello world! It is me, 000
Hello world! It is me, 001
Hello world! It is me, 006
Hello world! It is me, 003
Hello world! It is me, 002
Hello world! It is me, 005
Hello world! It is me, 004
Hello world! It is me, 007

----

And may even look like this 
----
main: creating thread 000
main: creating thread 001
main: creating thread 002
main: creating thread 003
Hello world! It is me, 000
Hello world! It is me, 001
Hello world! It is me, 003
Hello world! It is me, 002
main: creating thread 004
main: creating thread 005
main: creating thread 006
main: creating thread 007
Hello world! It is me, 006
Hello world! It is me, 005
Hello world! It is me, 004
Hello world! It is me, 007

----



=== Writing Multithreaded Programs: Structured or Implicit Multithreading


Interface such as Pthreads enable the programmer to create a wide
variety of multithreaded computations that can be structured in many
different ways. Large classes of interesting multithreaded
computations, however, can be expcessed using a more structured
approach, where threads are restricted in the way that they
synchronize with other threads.  One such interesting class of
computations is fork-join computations where a thread can spawn or
"fork" another thread or "join" with another existing thread.  Joining
a thread is the only mechanism through which threads synchronize.  The
figure below illustrates a fork-join computation.  The main threads
forks thread A, which then spaws thread B.  Thread B then joins thread
A, which then joins Thread M.


[[multithreading:general, Figure ?]]
.A multithreaded fork-join computation.
image::fork-join-threads.png["A multithreaded, fork-join computation.",width="500pt",align="center"]


In addition to fork-join, there are other interfaces for structured
multithreading such as async-finish, and futures.  These interfaces
are adopted in many programming languages: the
https://www.google.com/search?q=Cilk+language&gws_rd=ssl[Cilk
language] is primarily based on fork-join but also has some limited
support for async-finish;
https://www.google.com/search?q=X10&gws_rd=ssl[X10 language] is
primarily based on async-finish but also supports futures; the Haskell
language
https://www.google.com/search?q=Haskell+language&gws_rd=ssl[Haskell
language] provides support for fork-join and futures as well as
others;
https://www.google.com/search?q=Parallel+ML&gws_rd=ssl[Parallel ML]
language as implemented by the Manticore project is primarily based on
fork-join parallelism.
Such languages are sometimes called *_implicitly parallel_*.


The class computations that can be expressed as fork-join and
async-finish programs are sometimes called *_nested parallel_*.  The
term "nested" refers to the fact that a parallel computation can be
nested within another parallel computation.  This is as opposed to
*_flat parallelism_* where a parallel computation can only perform
sequential computations in parallel.  Flat parallelism used to be
common technique in the past but becoming increasingly less prominent.

/////
WHERE DOES NESL FIT IN THIS?  

WAS NESL THE INVENTOR OF DATA PARALLELISM? 

HOW DOES IT COMPARE TO MULTILISP?

////


=== Parallelism versus concurrency

	
Structured multithreading offers important benefits both in terms of
efficiency and expressiveness.  Using programming constructs such as
fork-join and futures, it is usually possible to write parallel
programs such that the program accepts a "sequential semantics" but
executes in parallel.  The sequential semantics enables the programmer
to treat the program as a serial program for the purposes of
correctness.  A run-time system then creates threads as necessary to
execute the program in parallel.  This approach offers is some ways
the best of both worlds: the programmer can reason about correctness
sequentially but the program executes in parallel.  The benefit of
structured multithreading in terms of efficiency stems from the fact
that threads are restricted in the way that they communicate.  This
makes it possible to implement an efficient run-time system.

More precisely, consider some sequential language such as the untyped
(pure) lambda calculus and its sequential dynamic semantics specified as a
strict, small step transition relation.  We can extend this language
with the structured multithreading by enriching the syntax language
with "fork-join" and "futures" constructs.  We can now extend the
dynamic semantics of the language in two ways: 1) trivially ignore
these constructs and execute serially as usual, and 2) execute in
parallel by creating parallel threads.  We can then show that these
two semantics are in fact identical, i.e., that they produce the same
value for the same expressions.  In other words, we can extend a rich
programming language with fork-join and futures and still give the
language a sequential semantics.  This shows that structured
multithreading is nothing but an efficiency and performance concern;
it can be ignored from the perspective of correctness.

We use the term *_parallelism_* to refer to the idea of computing in
parallel by using such structured multithreading constructs.  As we
shall see, we can write parallel algorithms for many interesting
problems. 
While parallel algorithms or applications constitute a large class,
they don't cover all applications.  Specifically applications that can
be expressed by using richer forms of multithreading such as the one
offered by Pthreads do not always accept a sequential semantics. In
such *_concurrent_* applications, threads can communicate and
coordinate in complex ways to accomplish the intended result.  A
classic concurrency example is the "producer-consumer problem", where
a consumer and a producer thread coordinate by using a fixed size
buffer of items.  The producer fills the buffer with items and the
consumer removes items from the buffer and they coordinate to make
sure that the buffer is never filled more than it can take.  We can
use operating-system level processes instead of threads to implement
similar concurrent applications.

In summary, parallelism is a property of the hardware or the software
platform where the computation takes place, whereas concurrency is a
property of the application.  Pure parallelism can be ignored for the
purposes of correctness; concurrency cannot be ignored for
understanding the behavior of the program. 

Parallelism and concurrency are orthogonal dimensions in the space of
all applications.  Some applications are concurrent, some are not.
Many concurrent applications can benefit from parallelism.  For
example, a browser, which is a concurrent application itself as it may
use a parallel algorithm to perform certain tasks.  On the other hand,
there is often no need to add concurrency to a parallel application,
because this unnecessarily complicates software.  It can, however,
lead to improvements in efficiency.

The following quote from Dijkstra suggest pursuing the approach of
making parallelism just a matter of execution (not one of semantics),
which is the goal of the much of the work on the development of
programming languages today. Note that in this particular quote,
Dijkstra does not mention that parallel algorithm design requires
thinking carefully about parallelism, which is one aspect where
parallel and serial computations differ.

/////
Note that Dijkstra uses the term "concurrency" to refer to
simultaneous execution of instructions by the machine. 
/////

[quote, Edsger W. Dijkstra, Selected Writings on Computing: A Personal Perspective (EWD 508)]
From the past terms such as "sequential programming" and "parallel
programming" are still with us, and we should try to get rid of them,
for they are a great source of confusion.  They date from the period
that it was the purpose of our programs to instruct our machines, now
it is the purpose of the machines to execute our programs.  Whether
the machine does so sequentially, one thing at a time, or with
considerable amount of concurrency, is a matter of implementation, and
should _not_ be regarded as a property of the programming language.  










