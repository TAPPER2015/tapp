[[ch:mutual-exclusion, Critical Sections and Mutual Exclusion]]
[[ch:race-conditions, Critical Sections and Mutual Exclusion]]
Critical Sections and Mutual Exclusion
--------------------------------------

In a multithreaded program, a *_critical section_* is a part of the
program that may not be executed by more than one thread at the same
time. Critical sections typically contain code that alters shared
objects, such as shared (e.g., global) variables.  This means that the
a critical section requires *_mutual exclusion_*: only one thread can
be inside the critical section at any time. 

//////////////////////////////
WARNING: Naama: the phrase “only one thread can be inside/enter the critical section at the same time” appears 4 times in these two paragraphs; sounds a bit odd.
//////////////////////////////

Since only one thread can be inside a critical section at a time,
threads must coordinate to make sure that they don't enter the
critical section at the same time.  If threads do not coordinate and
multiple threads enter the critical section at the same time, we say
that a *_race condition_* occurs, because the outcome of the program
depends on the relative timing of the threads, and thus can vary from
one execution to another.  Race conditions are sometimes benign but
usually not so, because they can lead to incorrect behavior.
Spectacular examples of race conditions' effects include the
"Northeast Blackout" of 2003, which affected 45 million people in the
US and 10 million people in Canada. 

It can be extremely difficult to find a race condition, because of the
non-determinacy of execution.  A race condition may lead to an
incorrect behavior only a tiny fraction of the time, making it
extremely difficult to observe and reproduce it.  For example, the
software fault that lead to the Northeast blackout took software
engineers "weeks of poring through millions of lines of code and data
to find it" according to one of the companies involved.


The problem of designing algorithms or protocols for ensuring mutual
exclusion is called the *_mutual exclusion problem_* or the *_critical
section_* problem.  There are many ways of solving instances of the
mutual exclusion problem. But broadly, we can distinguish two
categories: spin-locks and blocking-locks.  The idea in *_spin locks_*
is to busy wait until the critical section is clear of other threads.
Solutions based on *_blocking locks_* is similar except that instead
of waiting, threads simply block.  When the critical section is clear,
a blocked thread receives a signal that allows it to proceed.  The
term *_mutex_*, short for "mutual exclusion" is sometimes used to
refer to a lock.

Mutual exclusions problems have been studied extensively in the
context of several areas of computer science.  For example, in
operating systems research, processes, which like threads are
independent threads of control, belonging usually but not always to
different programs, can share certain systems' resources.  To enable
such sharing safely and efficiently, researchers have proposed various
forms of locks such as *_semaphores_*, which accepts both a
busy-waiting and blocking semantics.  Another class of locks, called
*_condition variables_* enable blocking synchronization by
conditioning or the value of a variable.


Parallelism and Mutual Exclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In parallel programming, mutual exclusion problems do not have to
arise.  For example, if we program in a purely functional language
extended with structured multithreading primitives such as fork-join
and futures, programs remain purely functional and mutual-exclusion
problems, and hence race conditions, do not arise.  If we program in an
imperative language, however, where memory is always a shared
resource, even when it is not intended to be so, threads can easily
share memory objects, even unintentionally, leading to race
conditions.

.Writing to the same location in parallel.
============

In the code below, both branches of `fork2` are writing into `b`.
What should then the output of this program be?

[source, {cpp}]
----
long b = 0;

fork2([&] {
  b = 1;
}, [&] {
  b = 2;
});

std::cout << "b = " << std::endl;
----

At the time of the print, the contents of `b` is determined by the
last write.  Thus depending on which of the two branches perform the
write, we can see both possibilities:

Output:
----
b = 1
----

Output:
----
b = 2
----
=============


.Fibonacci
============

Consider the following alternative implementation of the Fibonacci
function.  By "inlining" the plus operation in both branches, the
programmer got rid of the addition operation after the `fork2`.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      result += fib_par_racy(n-1);
    },[&] {
      result += fib_par_racy(n-2);
    });
  }
  return result;
}
----

This code is not correct because it has a race condition.

================

As in the example shows, separate threads are updating the value
result but it might look like this is not a race condition because the
update consists of an addition operation, which reads the value and
then writes to it. The race condition might be easier to see if we
expand out the applications of the `+=` operator.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      long a1 = fib_par_racy(n-1);
      long a2 = result;
      result = a1 + a2;
    },[&] {
      long b1 = fib_par_racy(n-2);
      long b2 = result;      
      result = b1 + b2;
    });
  }
  return result;
}
----

When written in this way, it is clear that these two parallel threads
are not independent: they both read `result` and write to
`result`. Thus the outcome depends on the order in which these reads
and writes are performed, as shown in the next example.

.Execution trace of a race condition
====================================

The following table takes us through one possible execution trace of
the call `fib_par_racy(2)`. The number to the left of each instruction
describes the time at which the instruction is executed.  Note that
since this is a multithreaded program, multiple instructions can be
executed at the same time. The particular execution that we have in
this example gives us a bogus result: the result is 0, not 1 as it
should be.

[width="100%",options="header"]
|==========
| Time step  | Thread 1     | Thread 2
|1 | a1 = fib_par_racy(1)   | b2 = fib_par_racy(0)
|2 | a2 = result            | b3 = result
|3 | result = a1 + a2       | _
|4 | _                      | result = b1 + b2
|==========

The reason we get a bogus result is that both threads read the initial
value of result at the same time and thus do not see each others
write.  In this example, the second thread "wins the race" and writes
into `result`. The value 1 written by the first thread is effectively
lost by being overwritten by the second thread.

==========================



Synchronization Hardware
~~~~~~~~~~~~~~~~~~~~~~~~

Since mutual exclusion is a common problem in computer science, many
hardware systems provide specific synchronization operations that can
help solve instances of the problem.  These operations may allow, for
example, testing the contents of a (machine) word then modifying it,
perhaps by swapping it with another word.  Such operations are
sometimes called atomic *_read-modify-write_* or *_RMW_*, for short,
operations.

A handful of different RMW operations have been proposed.  They
include operations such as *_load-link/store-conditional_*,
*_fetch-and-add_*, and *_compare-and-swap_*. They typically take the
memory location `x`, and a value `v` and replace the value of stored
at `x` with `f(x,v)`.  For example, the fetch-and-add operation takes
the location `x` and the increment-amount, and atomically increments
the value at that location by the specified amount, i.e., `f(x,v) = *x
+ v`.  

The compare-and-swap operation takes the location `x` and takes a pair
of values `(a,b)`, and stores `b` into `x` if the value in `x` is `a`,
i.e., `f(x,(a,b)) = if *x = a then b else a`; the operation returns a
Boolean indicating whether the operation successfully stored a new
value in `x`. The operation "compare-and-swap" is a reasonably
powerful synchronization operation: it can be used by arbitrarily many
threads to agree (reach consensus) on a value.  This instruction
therefore is frequently provided by modern parallel architectures such
as Intel's X86.  

In Clatexmath:[$++$], the `atomic` class can be used to perform synchronization.
Objects of this type are guarantee to be free of race conditions; and
in fact, in C++, they are the only objects that are guaranteed to be
free from race conditions.  The contents of an `atomic` object can be
accessed by `load` operations, updated by `store` operation, and also
updated by `compare_exchange_weak` and `compare_exchange_strong`
operations, the latter of which implement the compare-and-swap
operation.


.Accessing the contents of atomic memory cells
==========================
Access to the contents of any given cell is achieved by the `load()`
and `store()` methods.

[source,{cpp}]
----
std::atomic<bool> flag;

flag.store(false);
std::cout << flag.load() << std::endl;
flag.store(true);
std::cout << flag.load() << std::endl;
----

Output:

----
0
1
----

==========================

The key operation that help with race conditions is the
compare-and-exchange operation.

.Definition: compare and exchange
************

When executed with a `target` atomic object and an `expected` cell and
a new value `new' this operation performs the following steps,
atomically:

. Read the contents of `target`.
. If the contents equals the contents of `expected`,
then writes `new` into the `target` and returns `true`.
. Otherwise, returns `false` after updating the contents of the cell
`expected` with the contents of the target (exchange).
************

.Reading and writing atomic objects
==========================
[source,{cpp}]
----
std::atomic<bool> flag;

flag.store(false);
bool expected = false;
bool was_successful = flag.compare_exchange_strong(expected, true);
std::cout << "was_successful = " << was_successful
  				<< "; flag = " << flag.load()
					<< "; expected = " << expected
					<< std::endl;

bool expected2 = false;
bool was_successful2 = flag.compare_exchange_strong(expected2, true);
std::cout << "was_successful2 = " << was_successful2
					<< "; flag = " <<	flag.load()
					<< "; expected = " << expected2
					<< std::endl;
----

Output:

----
was_successful = 1; flag = 1; expected = 0
was_successful2 = 0; flag = 1; expected2 = 1
----

==========================

As another example use of the `atomic` class, recall our Fibonacci
example with the race condition.  In that example, race condition
arises because of concurrent writes to the `result` variable.  We can
eliminate this kind of race condition by using different memory
locations, or by using an atomic class and using a
`compare_exchange_strong` operation.

.Fibonacci
============

The following implementation of Fibonacci is not safe because the
variable `result` is shared and updated by multiple threads.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      result += fib_par_racy(n-1);
    },[&] {
      result += fib_par_racy(n-2);
    });
  }
  return result;
}
----

We can solve this problem by declaring `result` to be an atomic type
and using a standard busy-waiting protocol based on compare-and-swap.

[source, {cpp}]
----

long fib_par_atomic(long n) {
  atomic<long> result = 0;
  if (n < 2) {
    result.store(n);
  } else {
    fork2([&] {
      long r = fib_par_racy(n-1);
      // Atomically update result.
      while (true) {
        long exp = result.load();
        bool flag = result.compare_exchange_strong(exp,exp+r)
        if (flag) {break;}
      }
    },[&] {
      long r = fib_par_racy(n-2);
      // Atomically update result.
      while (true) {
        long exp = result.load();
        bool flag = result.compare_exchange_strong(exp,exp+r)
        if (flag) {break;}
      }
    });
  }
  return result;
}
----

The idea behind the solution is to load the current value of `result`
and atomically update `result` only if it has not been modified (by
another thread) since it was loaded.  This guarantees that the
`result` is always updated (read and modified) correctly without
missing an update from another thread.

//////////////////////////////////////////////////////////////////////
WARNING: Naama: Is fetch-and-add a hardware primitive? If so, fetch-and-add would solve the problem in the fib example without the busy-wait loop; F&A always succeeds.
/////////////////////////////////////////////////////////////////////

================

The example above illustrates a typical use of the compare-and-swap
operation.  In this particular example, we can probably prove our code
is correct.  But this is not always as easy due to a problem called
the "ABA problem."




.Spin Locks with Compare-And-Exchange
====

We can implement a spin lock using compare-and-exchange.  Spin locks
can ensure exclusive access to a shared resource such as a memory cell
or a file. For example, suppose that multiple threads wants to print
to the Standard IO. We could write such a program as follows.


[source, {cpp}]
----
std::atomic<bool> BigLock;

struct args {
  int tid;
};

void *hello(void *a)
{  
  args* myargs = (args*) a;
	int tid = myargs->tid;

  cout << "Hello world! It is me, Thread " << tid << endl;

  pthread_exit(NULL);
}

int main ()
{
  pthread_t threads[NTHREADS];
	
  for(int i=0; i < NTHREADS; i++ ){
		args* a = new args;
		a->tid = i;
		
    cout << "main: creating thread 00" << i << endl;

    int error = pthread_create(&threads[i], NULL, hello, (void *) a);
    if (error) {
      cout << "Error: unable to create thread," << error << endl;
      exit(-1);
    }
  }
  pthread_exit(NULL);
}
----

Since the Standard IO is a shared resource, the multiple threads can
write to it at the same time, resulting in garbled output.  The
example run below uses just two processors.

----
bash-3.2$ g++ -std=c++11  biglock-driver.cpp
bash-3.2$ ./a.out
main: creating thread 000
main: creating thread 001
main: creating thread 002
Hello world! It imsHHa eeimllnell:,oo    cTwwrhooerrraelltaddid!!n   g0II 
ttt  hiirsse  ammdee ,,0  0TT3hh
rreeaadmdH a e1i2l
n
l:o  cwroeraltdi!n gI tt hirse amde ,0 0T4h
read 3
Hmealilno:  wcorreladt!i nIgt  tihsr emaed,  0T0h5r
ead 4
Hello world! It is me, Thread 5
main: creating thread 006
main: creating thread 007
Hello world! It is me, Thread 6
Hello world! It is me, Thread 7
bash-3.2$ 
----

To ensure that each thread gets exclusive access to the standard IO,
we can use a spin lock, where each thread waits for other threads to
exit the critical section, where they access the Standard IO.  To
wait, a thread simply "spins" while checking the lock.  Before
entering its critical section, each thread takes the lock and upon
exiting it releases the lock. 


[source, {cpp}]
----
std::atomic<bool> BigLock;


/* Take BigLock */
void takeBigLock () {
  while (1) {
    bool flag = false;
    if (BigLock.compare_exchange_strong(flag,true)) {
		  return;
		}		
	}
}

/* ReleaseBig BigLock */
void releaseBigLock () {
  while (1) {
    bool flag = true;
    if (BigLock.compare_exchange_strong(flag,false)) {
		  return;
		}		
	}
}

void *hello(void *a)
{  
  args* myargs = (args*) a;
	int tid = myargs->tid;

	takeBigLock ();
  cout << "Hello world! It is me, Thread " << tid << endl;
	releaseBigLock ();

  pthread_exit(NULL);
}

int main ()
{
  pthread_t threads[NTHREADS];
  BigLock.store(false);
	
  for(int i=0; i < NTHREADS; i++ ){
		args* a = new args;
		a->tid = i;
		
  	takeBigLock ();
    cout << "main: creating thread 00" << i << endl;
    releaseBigLock ();
    int error = pthread_create(&threads[i], NULL, hello, (void *) a);
    if (error) {
      cout << "Error: unable to create thread," << error << endl;
      exit(-1);
    }
  }
  pthread_exit(NULL);
}
----

Here is an example run of the program.

[source, {cpp}]
----

bash-3.2$ g++ -std=c++11  biglock-driver.cpp
bash-3.2$ ./a.out
main: creating thread 000
main: creating thread 001
main: creating thread 002
Hello world! It is me, Thread 0
main: creating thread 003
Hello world! It is me, Thread 1
main: creating thread 004
Hello world! It is me, Thread 3
Hello world! It is me, Thread 2
main: creating thread 005
Hello world! It is me, Thread 4
main: creating thread 006
Hello world! It is me, Thread 5
main: creating thread 007
Hello world! It is me, Thread 6
Hello world! It is me, Thread 7

----
Note that the threads do not necessarily run in order even on to cores
but their messages are printed correctly, without being garbled.


//////////////////////////////////////////////////////////////////////
WARNING: Naama: Is fetch-and-add a hardware primitive? If so, fetch-and-add would solve the problem in the fib example without the busy-wait loop; F&A always succeeds.
/////////////////////////////////////////////////////////////////////

====


Nonblocking Concurrent Data Structures 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some cases, we have multiple threads operating on a shared data
structure such that the updates of one thread become visible to
others.  We refer to such data structures as *_concurred data
structures_*.  For example, we may have multiple threads using a stack
or a queue data structure to send and receive items between each
other.

The crux of the problem of designing a concurrent data structure is
ensuring correctness without penalizing performance.  Indeed, if we
don't care about performance, it is trivial to implement a concurrent
data structure by using a lock for the data structure such that any
thread that wants to use the data structure takes a lock for it before
using it, using it, and releases the lock afterwards.  In some cases,
this is all we can do, e.g., the accesses to the Standard IO in our
prior example cannot be handled differently without using a different,
perhaps a more "finer grain" interface for IO,  But in many cases,
there is a lot that can be done to ensure good performance.  

One the most commonly used techniques is to use non-blocking atomic
read-modify-write operations such as compare-and-exchange to implement
concurrent data structures.  In this approach, since threads never
block when using locks, a thread cannot prevent another thread from
making progress by taking a lock and blocking on the lock, hence the
name "non-blocking".  A non-blocking data structures is called *_lock
free_* if system-wide progress can be guaranteed, even as threads are
suspended.  A non-blocking data structure is called *_wait free_* if
per-thread progress can also be guaranteed.


A non-blocking stack data structure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose that we wish to implement a concurrent stack data structure
that can be shared my multiple threads.  The code below shows the code
for a standard serial "integer" stack that can hold integer values in
its nodes.

----
#include <iostream>
#include <cstdlib>

using namespace std;

class Node {
public:
  int value;
  Node* next;

  Node (int v) {
    value = v;
    next = NULL;
  }
};

class Stack {
public:
  Node* top;

  Stack () {
    top = NULL;
  };

  int pop ();
  void push (int v);
};

int Stack::pop () {
  if (top == NULL) {
    cout << "Stack is Empty" << endl;
    return -12;
  }
  else {
    int oldTop = top->value;
    top = top->next;
    return oldTop;
  }
}

void Stack::push (int value) {
  top = new Node (value,top); 
  return ;  
}
----

Allowing such a data structure by multiple threads leads to race
conditions and thus accesses must be protected by enforcing mutual
exclusion.  An example where multiple threads are operating on a
shared stack is shown below.

----
#include <iostream>
#include <cstdlib>
#include <pthread.h>
#include <atomic>
#include "nb-stack-int.h"

using namespace std;

#define NTHREADS 8
#define NPUSHPOP 2

struct args {
  int tid;
  Stack* s;
};


void *pushPop(void *a)
{  
  args* myargs = (args*) a;
  int tid = myargs->tid;
  Stack* sharedStack = myargs->s;

  cout << "Hello world! It is me, 00" << tid << endl;
  
  for (int i = 0; i < NPUSHPOP; ++i) {
    int j = NPUSHPOP * tid + i;
    sharedStack->push (NPUSHPOP * tid + i);
    sleep (1);
    int k = sharedStack->pop ();
    sleep (1);
  }

  pthread_exit(NULL);
}

int main ()
{
  pthread_t threads[NTHREADS];
  Stack* sharedStack = new Stack (); 
  
  int rc;
  for(int i=0; i < NTHREADS; i++ ){
    args* a = new args;
    a->tid = i;
    a->s = sharedStack;

    int error = pthread_create(&threads[i], NULL, pushPop, (void *) a);
    if (error) {
      cout << "Error: unable to create thread," << error << endl;
      exit(-1);
    }
  }
  pthread_exit(NULL);
}
----

The easiest way to ensure mutual exclusion is to use a lock to guard
accesses to the shared data structure, as in our Standard IO example
discussed earlier.  Such a lock, however, serializes all accesses to
the data structure, turning the data structure into a bottleneck.
Instead, we would like to allow the threads to operate concurrently
but without breaking the correctness of the stack. 

We can use the compare-and-exchange operation that we saw earlier to
achieve this.  The idea is to identify the parts of the code that
require mutually exclusive access to certain data and make sure that
such data are updated atomically.  The code below show an
implementation of stacks that attempts to guarantee mutual exclusion.
In function `push`, we first set up the new node, `u`, that we want to
add by using the current value of `top`.  We then update `top`, only
if it has not changed since we have read it by using
`compare_exchange_strong`.  Similarly, in function `pop`, we read the
current value of `top` into a variable `oldTop` and then create the
new value for top, `next`.  We then replace `top` with `next` by using
a `compare_exchange_strong` only if `top` has not changed since we
last read it.

----
#include <iostream>
#include <cstdlib>
#include <atomic>

using namespace std;

class Node {
public:
  int value;
  Node* next;

  Node (int v) {
    value = v;
    next = NULL;
  }

};

class Stack {
public:
  std::atomic<Node*> top;

  Stack () {
    top = NULL;
  };

  int pop ();
  void push (int v);
};


int Stack::pop () {
  if (top.load() == NULL) {
    cout << "Stack is Empty" << endl;
    return -12;
  }
  else {
    while (1) { 
      Node* oldTop = top.load();
      int oldTopValue = oldTop->value;
      Node* next = oldTop->next;
      
      if (top.compare_exchange_strong(oldTop,next)) {
        return oldTopValue;
      }
    }
  }
}

void Stack::push(const int value) 
{ 
  Node *u = new Node(value); 
  while (1) { 
    u->next = top.load();
    if (top.compare_exchange_strong(u->next, u)) { 
      break;
    }
  }
}
---- 



ABA problem



----
#include <iostream>
#include <cstdlib>
#include <atomic>

using namespace std;

class Node {
public:

  int value;
  Node* next;

  Node (int v) {
    value = v;
    next = NULL;
  }

  Node (int v, Node* u) {
    value = v;
    next = u;
  }
};

class Stack {
public: 
  struct state_t {
    Node* top;
    int64_t nPops;
  };

  std::atomic<state_t> state;

  Stack () {
    state_t s;
    s.top = NULL;
    s.nPops = 0;      
    state.store (s);
  };
  
  int pop ();
  void push (int v);
};

int Stack::pop () {
  state_t oldState = state.load();

  if (oldState.top == NULL) {
    cout << "Stack is Empty" << endl;
    return -12;
  }
  else {
    while (1) { 
      oldState = state.load ();
      Node* oldTop = oldState.top;
      int oldTopValue = oldTop->value;
      Node* next = oldTop->next;

      /* new state */
      state_t newState;
      newState.top = next;
      newState.nPops = oldState.nPops+1;
      
      if (state.compare_exchange_strong(oldState,newState)) {       
        return oldTopValue;
      }
    }
  }
}

void Stack::push(const int value) 
{ 
  Node *u = new Node(value); 

  while (1) { 
    state_t oldState = state.load ();
    u->next = oldState.top;
    
    /* new state */
    state_t newState;
    newState.top = u;
    newState.nPops = oldState.nPops;      
    if (state.compare_exchange_strong(oldState, newState)) { 
      break;
    }
  }
}

----


ABA problem 
~~~~~~~~~~~

While reasonably powerful, compare-and-swap suffers from the so-called
*_ABA_* problem.  To see this consider the following scenario where a
shared variable `result` is update by multiple threads in parallel: a
thread, say latexmath:[$T$], reads the `result` and stores its current
value, say `2`, in `current`.  In the mean time some other thread also
reads `result` and performs some operations on it, setting it back to
`2` after it is done.  Now, thread latexmath:[$T$] takes its turn
again and attempts to store a new value into `result` by using `2` as
the old value and being successful in doing so, because the value
stored in `result` appears to have not changed. The trouble is that
the value has actually changed and has been changed back to the value
that it used to be.  Thus, compare-and-swap was not able to detect
this change because it only relies on a simple shallow notion of
equality.  If for example, the value stored in `result` was a pointer,
the fact that the pointer remains the same does not mean that values
accessible from the pointer has not been modified; if for example, the
pointer led to a tree structure, an update deep in the tree could
leave the pointer unchanged, even though the tree has changed.

This problem is called the *_ABA_* problem, because it involves
cycling the atomic memory between the three values latexmath:[$A$],
latexmath:[$B$], and again latexmath:[$A$]).  The ABA problem is an
important limitation of compare-and-swap: the operation itself is not
atomic but is able to behave as if it is atomic if it can be ensured
that the equality test of the subject memory cell suffices for
correctness.

In the example below, ABA problem may happen (if the counter is
incremented and decremented again in between a load and a store) but
it is impossible to observe because it is harmless.  If however, the
compare-and-swap was on a memory object with references, the ABA
problem could have had observable effects.

The *_ABA_* problem can be exploited to give seemingly correct
implementations that are in fact incorrect.  To reduce the changes of
bugs due to the ABA problem, memory objects subject to
compare-and-swap are usually tagged with an additional field that
counts the number of updates.  This solves the basic problem but only
up to a point because the counter itself can also wrap around.  The
load-link/store-conditional operation solves this problem by
performing the write only if the memory location has not been updated
since the last read (load) but its practical implementations are hard
to come by.

//////////////////////////////////////////////////////////////////
WARNING: Naama: LL/SC behaves exactly like compare-and-swap, but does not suffer from the ABA problem (will fail if the value has been changed between a load-link and a store-conditional operation). Worth mentioning? Also, LL/SC can be implemented from CAS operations in constant time (maybe about 20 CAS operations), but this implementation is complicated and is not generally used in practice as far as I know.
///////////////////////////////////////////////////////////////////

