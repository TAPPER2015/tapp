== Chapter: Structured Multithreading

Multithreading interfaces such as Pthreads enable the programmer to
create a wide variety of multithreaded computations that can be
structured in many different ways.  Multithreaded programs, however,
can be extremely difficult to reason about, get right (correct), and
make efficient, because they could require reasoning about
exponentially many interleavings of code.  

Consider as a toy multithreaded program consisting of 16 threads, each
of which runs a 100 line program.  To reason about the correctness of
a program, we might have to reason about at least latexmath:[$16^100$]
different interleavings.  This is a huge number, larger than the
number of atoms in the known universe.  All evidence suggests that
human beings are not able to reason about such a large number of
possibilities.  Perhaps for this reason, multithreaded programs are
known to be notoriously difficult to get right.  Common problem
include

. reasoning about the correctness shared state and absence of race
conditions,

. reasoning about efficiency of concurrent data structures, and

. reasoning about correctness of memory management in non-GC'ed
programming languages.

For example, over the past two decades researchers have developed many
concurrent data structures, ranging from simple counters to more
complex structures such as concurrent queues and stacks.  Such data
structures have proved to be very difficult to establish correct; in
fact many were found to be incorrect after publication.  The
asymptotic efficiency of realistic concurrent data structures have
also turned out to be difficult to establish, with relatively few
tight bounds.

Fortunately, large classes of interesting multithreaded computations,
can be written using a more structured approach, where threads are
restricted in the way that they synchronize with other threads.  One
such interesting class of computations is fork-join computations where
a thread can spawn or "fork" another thread or "join" with another
existing thread.  Joining a thread is the only mechanism through which
threads synchronize.  The figure below illustrates a fork-join
computation.  The main threads forks thread A, which then spawns thread
B.  Thread B then joins thread A, which then joins Thread M.

[[multithreading:general, Figure ?]]
.A multithreaded fork-join computation.
image::fork-join-threads.png["A multithreaded, fork-join computation.",width="500pt",align="center"]


In addition to fork-join, there are other interfaces for structured
multithreading such as async-finish, and futures.  As in fork-join,
these approaches provide the programmer with a simple interface for
expressing concurrency by restricting the manner in which threads can
synchronize.  

One way provide language support for structured multithreading is to
simply use a more general threading library such as pThreads.
Although it is correct, this approach would be grossly inefficient
because structured multithreading can be implemented more efficiently.
Indeed, it has been adopted and implemented specifically in many
programming languages: the
https://www.google.com/search?q=Cilk+language&gws_rd=ssl[Cilk
language] is primarily based on fork-join but also has some limited
support for async-finish;
https://www.google.com/search?q=X10&gws_rd=ssl[X10 language] is
primarily based on async-finish but also supports futures; 
https://www.google.com/search?q=X10&gws_rd=ssl[Fork Join Java] 
and
https://www.google.com/search?q=X10&gws_rd=ssl[Habanero Java]  
extend the Java language with structured multithreading; and 
https://www.google.com/search?q=Haskell+language&gws_rd=ssl[Haskell
language] provides support for fork-join and futures as well as
others;
https://www.google.com/search?q=Parallel+ML&gws_rd=ssl[Parallel ML]
language as implemented by the Manticore project and by the Steel
project at CMU is primarily based on fork-join parallelism.  Such
languages are sometimes called *_implicitly parallel languages_*.

The class of computations that can be expressed as fork-join and
async-finish programs are sometimes called *_nested parallel_*.  The
term "nested" refers to the fact that a parallel computation can be
nested within another parallel computation.  This is as opposed to
*_flat parallelism_* where a parallel computation can only perform
sequential computations in parallel.  Flat parallelism used to be
common technique in the past but becoming increasingly less prominent.

The class of computations that can be expressed with futures is
sometimes called *_pipelined parallelism_*.  In this course, we will
not discuss futures further.

/////
WHERE DOES NESL FIT IN THIS?  

WAS NESL THE INVENTOR OF DATA PARALLELISM? 

HOW DOES IT COMPARE TO MULTILISP?

////


=== Parallelism versus concurrency

Structured multithreading offers important benefits both in terms of
efficiency and expressiveness.  Using programming constructs such as
fork-join and futures, it is possible to write parallel programs such
that the program accepts a "sequential semantics" but executes in
parallel.  The sequential semantics enables the programmer to treat
the program as a serial program for the purposes of correctness.  A
run-time system then creates threads as necessary to execute the
program in parallel. Since threads synchronize only in specific ways,
the run-time systems can optimize the creating and scheduling of
threads so as to maximize efficiency. 
Structured multithreading thus offers in some ways the best of both
worlds: the programmer can reason about correctness sequentially but
the program executes in parallel.  

More precisely, consider a purely functional sequential language such
as the untyped (pure) lambda calculus and its sequential dynamic
semantics specified as a strict, small step transition relation.  We
can extend this language with the structured multithreading by
enriching the syntax language with "fork-join" and "futures"
constructs.  We can now extend the dynamic semantics of the language
in two ways: 1) trivially ignore these constructs and execute serially
as usual, and 2) execute in parallel by creating parallel threads.
With some care, we can establish these two semantics to be identical,
i.e., they produce the same value for the same expressions.  In other
words, we can extend a rich programming language with fork-join and
futures and still give the language a sequential semantics.  This
shows that structured multithreading is nothing but an efficiency and
performance concern; it can be ignored from the perspective of
correctness.

We use the term *_parallelism_* to refer to the idea of computing in
parallel by using such structured multithreading constructs.  As we
shall see, we can write parallel algorithms for many interesting
problems.  Although parallel algorithms or applications constitute a
large class, they don't cover all applications.  Specifically
applications that can be expressed by using richer forms of
multithreading such as the one offered by Pthreads do not always
accept a sequential semantics. In such *_concurrent_* applications,
threads can communicate and coordinate in complex ways to accomplish
the intended result. One such example is the multithreaded program
that we considered for using a shared non-blocking stack.  Another
example, which is a classic concurrency example, is the
"producer-consumer problem", where a consumer and a producer thread
coordinate by using a fixed size buffer of items.  The producer fills
the buffer with items and the consumer removes items from the buffer
and they coordinate to make sure that the buffer is never filled more
than it can take.  Operating-system level processes sometimes
communicate similarly in some concurrent applications.

In summary, parallelism is a property of the hardware or the software
platform where the computation takes place, whereas concurrency is a
property of the application.  Pure parallelism can be ignored for the
purposes of correctness; concurrency cannot be ignored for
understanding the behavior of the program. 

Parallelism and concurrency are orthogonal dimensions in the space of
all applications.  Some applications are concurrent, some are not.
Many concurrent applications can benefit from parallelism.  For
example, a browser, which is a concurrent application itself as it may
use a parallel algorithm to perform certain tasks.  On the other hand,
there is usually no need to add concurrency to a parallel application,
because this unnecessarily complicates software.  It can, however,
lead to improvements in efficiency.

The following quote from Dijkstra suggest pursuing the approach of
making parallelism just a matter of execution (not one of semantics),
which is the goal of the much of the work on the development of
programming languages today. Note that in this particular quote,
Dijkstra does not mention that parallel algorithm design requires
thinking carefully about work and span, as opposed to just work as is
sequential computing.

/////
Note that Dijkstra uses the term "concurrency" to refer to
simultaneous execution of instructions by the machine. 
/////

[quote, Edsger W. Dijkstra, Selected Writings on Computing: A Personal Perspective (EWD 508)]
From the past terms such as "sequential programming" and "parallel
programming" are still with us, and we should try to get rid of them,
for they are a great source of confusion.  They date from the period
that it was the purpose of our programs to instruct our machines, now
it is the purpose of the machines to execute our programs.  Whether
the machine does so sequentially, one thing at a time, or with
considerable amount of concurrency, is a matter of implementation, and
should _not_ be regarded as a property of the programming language.  


Parallelism and Mutual Exclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In parallel programming, mutual exclusion problems do not have to
arise.  For example, if we program in a purely functional language
extended with structured multithreading primitives such as fork-join
and futures, programs remain purely functional and mutual-exclusion
problems, and hence race conditions, do not arise.  If we program in an
imperative language, however, where memory is always a shared
resource, even when it is not intended to be so, threads can easily
share memory objects, even unintentionally, leading to race
conditions.

.Writing to the same location in parallel.
============

In the code below, both branches of `fork2` are writing into `b`.
What should then the output of this program be?

[source, {cpp}]
----
long b = 0;

fork2([&] {
  b = 1;
}, [&] {
  b = 2;
});

std::cout << "b = " << std::endl;
----

At the time of the print, the contents of `b` is determined by the
last write.  Thus depending on which of the two branches perform the
write, we can see both possibilities:

Output:
----
b = 1
----

Output:
----
b = 2
----
=============


.Fibonacci
============

Consider the following alternative implementation of the Fibonacci
function.  By "inlining" the plus operation in both branches, the
programmer got rid of the addition operation after the `fork2`.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      result += fib_par_racy(n-1);
    },[&] {
      result += fib_par_racy(n-2);
    });
  }
  return result;
}
----

This code is not correct because it has a race condition.

================

As in the example shows, separate threads are updating the value
result but it might look like this is not a race condition because the
update consists of an addition operation, which reads the value and
then writes to it. The race condition might be easier to see if we
expand out the applications of the `+=` operator.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      long a1 = fib_par_racy(n-1);
      long a2 = result;
      result = a1 + a2;
    },[&] {
      long b1 = fib_par_racy(n-2);
      long b2 = result;      
      result = b1 + b2;
    });
  }
  return result;
}
----

When written in this way, it is clear that these two parallel threads
are not independent: they both read `result` and write to
`result`. Thus the outcome depends on the order in which these reads
and writes are performed, as shown in the next example.

.Execution trace of a race condition
====================================

The following table takes us through one possible execution trace of
the call `fib_par_racy(2)`. The number to the left of each instruction
describes the time at which the instruction is executed.  Note that
since this is a multithreaded program, multiple instructions can be
executed at the same time. The particular execution that we have in
this example gives us a bogus result: the result is 0, not 1 as it
should be.

[width="100%",options="header"]
|==========
| Time step  | Thread 1     | Thread 2
|1 | a1 = fib_par_racy(1)   | b2 = fib_par_racy(0)
|2 | a2 = result            | b3 = result
|3 | result = a1 + a2       | _
|4 | _                      | result = b1 + b2
|==========

The reason we get a bogus result is that both threads read the initial
value of result at the same time and thus do not see each others
write.  In this example, the second thread "wins the race" and writes
into `result`. The value 1 written by the first thread is effectively
lost by being overwritten by the second thread.

==========================

In the example, race condition arises because of concurrent writes to
the `result` variable.  We can eliminate this kind of race condition
by using different memory locations, or by using an atomic class and
using a `compare_exchange_strong` operation.

.Fibonacci
============

The following implementation of Fibonacci is not safe because the
variable `result` is shared and updated by multiple threads.

[source, {cpp}]
----
long fib_par_racy(long n) {
  long result = 0;
  if (n < 2) {
    result = n;
  } else {
    fork2([&] {
      result += fib_par_racy(n-1);
    },[&] {
      result += fib_par_racy(n-2);
    });
  }
  return result;
}
----

We can solve this problem by declaring `result` to be an atomic type
and using a standard busy-waiting protocol based on compare-and-swap.

[source, {cpp}]
----

long fib_par_atomic(long n) {
  atomic<long> result = 0;
  if (n < 2) {
    result.store(n);
  } else {
    fork2([&] {
      long r = fib_par_racy(n-1);
      // Atomically update result.
      while (true) {
        long exp = result.load();
        bool flag = result.compare_exchange_strong(exp,exp+r)
        if (flag) {break;}
      }
    },[&] {
      long r = fib_par_racy(n-2);
      // Atomically update result.
      while (true) {
        long exp = result.load();
        bool flag = result.compare_exchange_strong(exp,exp+r)
        if (flag) {break;}
      }
    });
  }
  return result;
}
----

The idea behind the solution is to load the current value of `result`
and atomically update `result` only if it has not been modified (by
another thread) since it was loaded.  This guarantees that the
`result` is always updated (read and modified) correctly without
missing an update from another thread.

//////////////////////////////////////////////////////////////////////

WARNING: Naama: Is fetch-and-add a hardware primitive? If so,
fetch-and-add would solve the problem in the fib example without the
busy-wait loop; F&A always succeeds.

/////////////////////////////////////////////////////////////////////

================

